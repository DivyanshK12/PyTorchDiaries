{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr = imageio.imread(r\"C:\\Users\\divya\\PyTorchDiaries\\data\\p1ch2\\bobby.jpg\")\n",
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img_arr)\n",
    "out = img.permute(2, 0, 1)\n",
    "#PyTorch requires channel, width , height order\n",
    "# Img io gave width, height, color/channel dimesnions\n",
    "# Changing a pixel in img will change out as still wokrs on same storage element with different size and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store a dataset of multiple images to use as an input for our neural networks\n",
    "# We store images in a batch along with the first dimension to obtain a N*C*H*W tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preallocating tensor and then fillit up with images from a dir\n",
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3 , 256, 256, dtype = torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = r\"C:\\Users\\divya\\PyTorchDiaries\\data\\p1ch4\\image-cats\"\n",
    "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == \".png\"]\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "    img_t = torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2, 0, 1)\n",
    "    img_t = img_t[:3] # Keeping RGB channels and removing channels like transperency etc\n",
    "    batch[i] = img_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.float()\n",
    "batch /= 255.0\n",
    "# convert to float and scale by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make mean 0 and unit standar dev\n",
    "n_channels = batch.shape[1]\n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:,c])\n",
    "    std = torch.std(batch[:,c])\n",
    "    batch[:, c] = (batch[:, c] - mean)/ std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1439,  0.0730, -0.4234,  ...,  0.0375,  0.0198,  0.1794],\n",
       "          [ 0.4631, -0.2461,  0.3035,  ..., -0.4944, -0.2107, -0.1752],\n",
       "          [-0.3703,  0.1439, -0.7249,  ..., -0.2993, -0.0866,  0.2858],\n",
       "          ...,\n",
       "          [-0.5653, -0.3171, -0.3348,  ..., -0.3703, -0.5298, -0.6362],\n",
       "          [-0.3348, -0.3171, -0.4412,  ..., -0.5830, -0.4766, -0.6007],\n",
       "          [-0.3348, -0.4412, -0.5298,  ..., -0.6185, -0.4766, -0.4944]],\n",
       "\n",
       "         [[ 0.4632,  0.3874, -0.1058,  ...,  0.3874,  0.3874,  0.6150],\n",
       "          [ 0.8615,  0.0839,  0.6529,  ..., -0.1816,  0.1408,  0.1787],\n",
       "          [-0.0299,  0.4822, -0.4661,  ...,  0.0649,  0.2736,  0.7098],\n",
       "          ...,\n",
       "          [-0.2954, -0.0868, -0.0678,  ...,  0.0460, -0.1247, -0.2196],\n",
       "          [-0.0678, -0.0678, -0.1627,  ..., -0.1627, -0.0489, -0.1816],\n",
       "          [-0.0678, -0.2006, -0.2385,  ..., -0.2196, -0.0868, -0.0678]],\n",
       "\n",
       "         [[ 0.7792,  0.6573,  0.1495,  ...,  0.8198,  0.8401,  1.1041],\n",
       "          [ 1.3072,  0.3933,  0.9417,  ...,  0.2308,  0.5761,  0.6167],\n",
       "          [ 0.2714,  0.8401, -0.2161,  ...,  0.4339,  0.6979,  1.1245],\n",
       "          ...,\n",
       "          [ 0.0480,  0.3526,  0.2917,  ...,  0.6979,  0.4948,  0.3526],\n",
       "          [ 0.3526,  0.3526,  0.1495,  ...,  0.3933,  0.5354,  0.3933],\n",
       "          [ 0.3323,  0.1495,  0.0886,  ...,  0.3526,  0.4948,  0.5151]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9595,  0.7999,  0.7467,  ..., -2.3915, -2.3915, -2.4092],\n",
       "          [ 0.9063,  0.7822,  0.7290,  ..., -2.3738, -2.3738, -2.3738],\n",
       "          [ 0.8886,  0.7999,  0.7113,  ..., -2.4092, -2.4092, -2.4092],\n",
       "          ...,\n",
       "          [-0.9731, -1.1681, -1.2745,  ..., -1.9837, -1.9837, -1.9837],\n",
       "          [-1.2922, -1.4163, -0.8312,  ..., -1.9837, -1.9837, -1.9660],\n",
       "          [-1.1149, -0.7958, -1.0263,  ..., -1.9837, -1.9660, -1.9482]],\n",
       "\n",
       "         [[ 0.6908,  0.4632,  0.3494,  ..., -2.0024, -2.0024, -2.0214],\n",
       "          [ 0.6908,  0.4822,  0.3684,  ..., -1.9645, -1.9645, -1.9645],\n",
       "          [ 0.7098,  0.5391,  0.3684,  ..., -1.9645, -1.9645, -1.9645],\n",
       "          ...,\n",
       "          [-1.0920, -1.3196, -1.4334,  ..., -1.6800, -1.6800, -1.6800],\n",
       "          [-1.5472, -1.6800, -1.0541,  ..., -1.6800, -1.6800, -1.6610],\n",
       "          [-1.4144, -1.0730, -1.3196,  ..., -1.6800, -1.6610, -1.6420]],\n",
       "\n",
       "         [[-0.4598, -0.7644, -0.9472,  ..., -1.7190, -1.7190, -1.7394],\n",
       "          [-0.4801, -0.7441, -0.9472,  ..., -1.7190, -1.7190, -1.7190],\n",
       "          [-0.4801, -0.7035, -0.9472,  ..., -1.7190, -1.7190, -1.7190],\n",
       "          ...,\n",
       "          [-1.2113, -1.4550, -1.5972,  ..., -1.4956, -1.4956, -1.4956],\n",
       "          [-1.6175, -1.8003, -1.1300,  ..., -1.4956, -1.4956, -1.4753],\n",
       "          [-1.4550, -1.0894, -1.3941,  ..., -1.4956, -1.4753, -1.4550]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5978,  1.5978,  1.5978,  ...,  1.1723,  1.1900,  1.1900],\n",
       "          [ 1.5978,  1.5978,  1.5978,  ...,  1.1723,  1.1900,  1.1900],\n",
       "          [ 1.5978,  1.5978,  1.5978,  ...,  1.1723,  1.1900,  1.1900],\n",
       "          ...,\n",
       "          [ 1.1723,  1.1545,  1.1368,  ...,  0.6936,  0.7467,  0.7999],\n",
       "          [ 1.1723,  1.1545,  1.1368,  ...,  0.6758,  0.7467,  0.7822],\n",
       "          [ 1.1723,  1.1545,  1.1368,  ...,  0.6758,  0.7467,  0.7822]],\n",
       "\n",
       "         [[ 1.5253,  1.5253,  1.5253,  ...,  1.1081,  1.1460,  1.1460],\n",
       "          [ 1.5253,  1.5253,  1.5253,  ...,  1.1081,  1.1460,  1.1460],\n",
       "          [ 1.5253,  1.5253,  1.5253,  ...,  1.1081,  1.1460,  1.1460],\n",
       "          ...,\n",
       "          [ 0.2546,  0.2356,  0.2167,  ..., -0.2765, -0.2196, -0.1627],\n",
       "          [ 0.2546,  0.2356,  0.2167,  ..., -0.2954, -0.2196, -0.1816],\n",
       "          [ 0.2546,  0.2356,  0.2167,  ..., -0.2954, -0.2196, -0.1816]],\n",
       "\n",
       "         [[ 0.9417,  0.9417,  0.9417,  ...,  0.6979,  0.7182,  0.7182],\n",
       "          [ 0.9417,  0.9417,  0.9417,  ...,  0.6979,  0.7182,  0.7182],\n",
       "          [ 0.9417,  0.9417,  0.9417,  ...,  0.6979,  0.7182,  0.7182],\n",
       "          ...,\n",
       "          [-0.2364, -0.2567, -0.2770,  ..., -0.5410, -0.4598, -0.3785],\n",
       "          [-0.2364, -0.2567, -0.2770,  ..., -0.5410, -0.4395, -0.3988],\n",
       "          [-0.2364, -0.2567, -0.2770,  ..., -0.5207, -0.4395, -0.3785]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Images : Volumetric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%3/99 files (3.0%6/99 files (6.1%8/99 files (8.1%10/99 files (10.1%13/99 files (13.1%15/99 files (15.2%17/99 files (17.2%20/99 files (20.2%23/99 files (23.2%25/99 files (25.3%27/99 files (27.3%29/99 files (29.3%31/99 files (31.3%34/99 files (34.3%37/99 files (37.4%39/99 files (39.4%40/99 files (40.4%43/99 files (43.4%46/99 files (46.5%49/99 files (49.5%52/99 files (52.5%55/99 files (55.6%58/99 files (58.6%61/99 files (61.6%62/99 files (62.6%64/99 files (64.6%65/99 files (65.7%68/99 files (68.7%71/99 files (71.7%73/99 files (73.7%76/99 files (76.8%79/99 files (79.8%82/99 files (82.8%84/99 files (84.8%86/99 files (86.9%89/99 files (89.9%91/99 files (91.9%94/99 files (94.9%96/99 files (97.0%99/99 files (100.099/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 2/99  (2.05/99  (5.18/99  (8.111/99  (11.115/99  (15.217/99  (17.221/99  (21.224/99  (24.227/99  (27.329/99  (29.332/99  (32.335/99  (35.437/99  (37.440/99  (40.443/99  (43.447/99  (47.551/99  (51.554/99  (54.557/99  (57.660/99  (60.664/99  (64.667/99  (67.769/99  (69.772/99  (72.774/99  (74.776/99  (76.878/99  (78.881/99  (81.883/99  (83.886/99  (86.989/99  (89.991/99  (91.994/99  (94.997/99  (98.099/99  (100.0%99/99  (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "dir_path = r\"C:\\Users\\divya\\PyTorchDiaries\\data\\p1ch4\\volumetric-dicom\\2-LUNG 3.0  B70f-04083\"\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layout is different than what pytorch expects as this has no channel information. \n",
    "# To make room for channel information use unsqueeze\n",
    "vol = torch.from_numpy(vol_arr).float()\n",
    "vol = torch.unsqueeze(vol, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 99, 512, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
